{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81edffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import time\n",
    "\n",
    "sys.path.append('/home/ysr/project/ai/deep_learning')\n",
    "\n",
    "from random import shuffle\n",
    "#import features.audio_features as features_lib\n",
    "import feature.audio_features as features_lib\n",
    "from utils.params import Params\n",
    "import utils.tools as tools\n",
    "#import utils.params as ynet_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dee9ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKGROUND_NOISE_DIR_NAME = '_background_noise_'\n",
    "SILENCE_LABEL = '_Silence_'\n",
    "UNKNOWN_WORD_LABEL = '_Unknown_'\n",
    "\n",
    "#设置行不限制数量\n",
    "pd.set_option('display.max_rows',None)\n",
    "#最后的的参数可以限制输出行的数量\n",
    "\n",
    "#设置列不限制数量\n",
    "pd.set_option('display.max_columns',None)\n",
    "#最后的的参数可以限制输出列的数量\n",
    "\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',1000)\n",
    "\n",
    "def log_mel_spectrogram(audio_file, param):\n",
    "    wav_data, sr = sf.read(audio_file, dtype=np.int16)\n",
    "    assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
    "\n",
    "    # 补0 to do  mask\n",
    "    if len(wav_data) < param.sample_rate:\n",
    "        wav_data = np.pad(wav_data, (0, int(param.sample_rate - len(wav_data))), 'constant', constant_values = 0)\n",
    "\n",
    "    waveform = wav_data / 32768.0\n",
    "\n",
    "    if len(waveform.shape) > 1:\n",
    "            waveform = np.mean(waveform, axis=1)  # 多通道转单通道\n",
    "\n",
    "    if sr != param.sample_rate:\n",
    "            waveform = resampy.resample(waveform, sr, param.sample_rate)\n",
    "\n",
    "    waveform = np.reshape(waveform, [1, -1]).astype(np.float32)\n",
    "    return features_lib.waveform_to_log_mel_spectrogram_patches(tf.squeeze(waveform, axis=0), param)\n",
    "\n",
    "def get_files_and_labels(train_dir, file_type = 'wav', train_split = 0.9, wanted_label = None):\n",
    "    #ignored = {\"folder_one\", \"folder_two\", \"folder_three\"}\n",
    "    #folders = [x for x in os.listdir(path) if x not in ignored]\n",
    "    \n",
    "    if not wanted_label:\n",
    "        classes = sorted(os.listdir(train_dir))\n",
    "    else :\n",
    "        classes = [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_label.split(',') \n",
    "    files_train = list()\n",
    "    files_val = list()\n",
    "    labels = dict()\n",
    " \n",
    "    for cnt, i in enumerate(classes): # loop over classes\n",
    "        tmp = os.listdir(train_dir + i)\n",
    "        shuffle(tmp)\n",
    "        for j in tmp[:round(len(tmp)*train_split)]: # loop over training samples\n",
    "            if j.split('.')[-1] == file_type:\n",
    "                files_train.append(train_dir + i +'/' + j)\n",
    "        for j in tmp[round(len(tmp)*train_split):]: # loop over validation samples\n",
    "            if j.split('.')[-1] == file_type:\n",
    "                files_val.append(train_dir + i +'/' + j)\n",
    "        labels[i] = cnt\n",
    "    return files_train, files_val, labels\n",
    "\n",
    "\n",
    "def _parse_audio_function(example_string):\n",
    "    n_classes = 527\n",
    "    feature = { \n",
    "        'patches': tf.io.FixedLenFeature([], tf.string),\n",
    "        'patches_shape': tf.io.FixedLenFeature(shape=(3,), dtype=tf.int64), # shape = 3 \n",
    "        'label': tf.io.FixedLenFeature([n_classes], dtype=tf.int64), \n",
    "    }       \n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature)\n",
    "    patches_raw = feature_dict['patches']   \n",
    "    patches_shape = feature_dict['patches_shape']\n",
    "    label = feature_dict['label']\n",
    "\n",
    "    patches = tf.io.decode_raw(patches_raw, tf.float32)\n",
    "    patches = tf.reshape(patches, patches_shape)\n",
    "    label = tf.reshape(label, (1, n_classes)) \n",
    "    return patches, label\n",
    "\n",
    "\n",
    "def audio_example(patches, label): # tf record example\n",
    "    feature = { \n",
    "        'patches': tools._numpy_float32_feature(patches),\n",
    "        'patches_shape': tools._shape_feature(patches.shape),\n",
    "        'label': tools._int64_list_feature(label),\n",
    "    }   \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)) \n",
    "\n",
    "def audio_file_example(patches): # tf record example\n",
    "    feature = { \n",
    "        'patches': tools._bytes_feature(patches),\n",
    "    }   \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)) \n",
    "\n",
    "class ESC50DataSet():\n",
    "    def __init__(self):\n",
    "        print('')\n",
    "\n",
    "    def build_dataset():\n",
    "        print('')\n",
    "\n",
    "class AudioSetDataSet():\n",
    "    def __init__(self, params, path, cache_dir = None):\n",
    "        self.params = params\n",
    "        self.path = path\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "    def __read_pd(self, path):\n",
    "        valid_meta = pd.read_csv(path + 'valid.csv')\n",
    "        valid_id = valid_meta.groupby('YTID')['YTID'].apply(lambda cat: cat.sample(1)).reset_index()['YTID']\n",
    "        train_meta = pd.read_csv(path+ 'train.csv')\n",
    "        train_id = train_meta.groupby('YTID')['YTID'].apply(lambda cat: cat.sample(1)).reset_index()['YTID']\n",
    "        class_meta = pd.read_csv(path + 'class_labels_indices.csv')\n",
    "        class_id = class_meta.groupby('index')['index'].apply(lambda cat: cat.sample(1)).reset_index()['index']\n",
    "        return valid_meta, valid_id, train_meta, train_id, class_meta, class_id\n",
    "        \n",
    "    def __build_classes(self, class_meta, class_id):\n",
    "        labels = []\n",
    "        label_index = {}\n",
    "        for index in range(len(class_id)):\n",
    "            item = []\n",
    "            clas = class_id[index]\n",
    "            clas = class_meta[class_meta.index == clas]\n",
    "            label_index[clas.mid.to_string(index=False)] = clas.index[0]\n",
    "            item.append(clas.index[0])\n",
    "            labels.append(item)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        mlb.fit(labels)\n",
    "        print(labels)\n",
    "        return label_index, mlb, mlb.classes_\n",
    "    \n",
    "\n",
    "                    \n",
    "    def __build_cache_np(self, wav_dir, npy_dir, cache_path, file_ytid, meta,  mlb, label_index, params):\n",
    "        total_size = len(file_ytid)\n",
    "        with tf.io.TFRecordWriter(cache_path) as writer:\n",
    "            for index in range(total_size):\n",
    "                label_id = []\n",
    "                item = file_ytid[index]\n",
    "                item = meta[meta.YTID == item]\n",
    "                \n",
    "                multi_label = item.positive_labels.to_string(index=False).split(\",\")\n",
    "                for label in multi_label:\n",
    "                    label_id.append(label_index[label])\n",
    "\n",
    "                wav_path = wav_dir + item.YTID.to_string(index=False) + '.wav'\n",
    "                patches_npy_path = npy_dir + 'patches/' + item.YTID.to_string(index=False) + '.npy'\n",
    "                label_npy_path = npy_dir + 'label/' + item.YTID.to_string(index=False) + '.npy'\n",
    "                \n",
    "                spectrogram, patches = log_mel_spectrogram(wav_path, params)\n",
    "                np.save(patches_npy_path, patches)\n",
    "                np.save(label_npy_path, mlb.transform([label_id])[0])\n",
    "                \n",
    "                d = f\"Scanning '{wav_path}' audio and labels... {total_size} found, {index} corrupted\"\n",
    "                tqdm(None, desc=d, total=total_size, initial=index)  # display cache results\n",
    "                if index == 10:\n",
    "                    break\n",
    "\n",
    "            writer.close()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        valid_meta, valid_id, train_meta, train_id, class_meta, class_id = self.__read_pd(self.path)\n",
    "        label_index, mlb, classes = self.__build_classes(class_meta, class_id)\n",
    "        self.__build_cache_np(self.path + 'train_wav/', self.path + 'train_np/', self.path + 'train.cache', train_id, train_meta, mlb, \n",
    "                                    label_index, self.params)\n",
    "        self.__build_cache_np(self.path + 'valid_wav/', self.path + 'valid_np/', self.path + 'valid.cache', valid_id, valid_meta, mlb, \n",
    "                                    label_index, self.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61551e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/--PJHxphWEs.wav' audio and labels... 19644 found, 0 corrupted:   0%|          | 0/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/--aE2O5G5WE.wav' audio and labels... 19644 found, 1 corrupted:   0%|          | 1/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/--aaILOrkII.wav' audio and labels... 19644 found, 2 corrupted:   0%|          | 2/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/--cB2ZVjpnA.wav' audio and labels... 19644 found, 3 corrupted:   0%|          | 3/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/--ekDLDTUXA.wav' audio and labels... 19644 found, 4 corrupted:   0%|          | 4/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-0DLPzsiXXE.wav' audio and labels... 19644 found, 5 corrupted:   0%|          | 5/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-0DdlOuIFUI.wav' audio and labels... 19644 found, 6 corrupted:   0%|          | 6/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-0O3e95y4gE.wav' audio and labels... 19644 found, 7 corrupted:   0%|          | 7/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-0SdAVK79lg.wav' audio and labels... 19644 found, 8 corrupted:   0%|          | 8/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-0mjrMposBM.wav' audio and labels... 19644 found, 9 corrupted:   0%|          | 9/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/train_wav/-11LhdJgBb8.wav' audio and labels... 19644 found, 10 corrupted:   0%|          | 10/19644 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/--4gqARaEJE.wav' audio and labels... 17979 found, 0 corrupted:   0%|          | 0/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/--BfvyPmVMo.wav' audio and labels... 17979 found, 1 corrupted:   0%|          | 1/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/--U7joUcTCo.wav' audio and labels... 17979 found, 2 corrupted:   0%|          | 2/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/--i-y1v8Hy8.wav' audio and labels... 17979 found, 3 corrupted:   0%|          | 3/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0BIyqJj9ZU.wav' audio and labels... 17979 found, 4 corrupted:   0%|          | 4/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0CamVQdP_Y.wav' audio and labels... 17979 found, 5 corrupted:   0%|          | 5/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0Gj8-vB1q4.wav' audio and labels... 17979 found, 6 corrupted:   0%|          | 6/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0RWZT-miFs.wav' audio and labels... 17979 found, 7 corrupted:   0%|          | 7/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0YUDn-1yII.wav' audio and labels... 17979 found, 8 corrupted:   0%|          | 8/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0jeONf82dE.wav' audio and labels... 17979 found, 9 corrupted:   0%|          | 9/17979 [00:00<?, ?it/s]\n",
      "Scanning '/home/ysr/dataset/audio/audioset/valid_wav/-0nqfRcnAYE.wav' audio and labels... 17979 found, 10 corrupted:   0%|          | 10/17979 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "param = Params()\n",
    "dataset = AudioSetDataSet(param, '/home/ysr/dataset/audio/audioset/', './')\n",
    "dataset.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9aa5ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_and_labels(train_dir, typ='npy', train_split=0.9):  \n",
    "    files_train = list()\n",
    "    files_val = list()\n",
    "    label_train = list()\n",
    "    label_val = list()\n",
    "    tmp = os.listdir(train_dir + 'label/')\n",
    "    shuffle(tmp)\n",
    "    \n",
    "    for j in tmp[:round(len(tmp)*train_split)]: # loop over training samples\n",
    "        if j.split('.')[-1] == typ:\n",
    "            files_train.append(train_dir  +'patches/' + j)\n",
    "            label_train.append(train_dir  +'label/' + j)\n",
    "    for j in tmp[round(len(tmp)*train_split):]: # loop over validation samples\n",
    "        if j.split('.')[-1] == typ:\n",
    "            files_val.append(train_dir +'patches/' + j)\n",
    "            label_val.append(train_dir  +'label/' + j)\n",
    "            \n",
    "    return files_train, files_val, label_train, label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b03fbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ysr/dataset/audio/audioset/train_np/patches/--cB2ZVjpnA.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-0SdAVK79lg.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/--aaILOrkII.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-11LhdJgBb8.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/--aE2O5G5WE.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-0mjrMposBM.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-0DdlOuIFUI.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-0O3e95y4gE.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/-0DLPzsiXXE.npy', '/home/ysr/dataset/audio/audioset/train_np/patches/--ekDLDTUXA.npy']\n",
      "['/home/ysr/dataset/audio/audioset/train_np/patches/--PJHxphWEs.npy']\n",
      "['/home/ysr/dataset/audio/audioset/train_np/label/--cB2ZVjpnA.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-0SdAVK79lg.npy', '/home/ysr/dataset/audio/audioset/train_np/label/--aaILOrkII.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-11LhdJgBb8.npy', '/home/ysr/dataset/audio/audioset/train_np/label/--aE2O5G5WE.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-0mjrMposBM.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-0DdlOuIFUI.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-0O3e95y4gE.npy', '/home/ysr/dataset/audio/audioset/train_np/label/-0DLPzsiXXE.npy', '/home/ysr/dataset/audio/audioset/train_np/label/--ekDLDTUXA.npy']\n",
      "['/home/ysr/dataset/audio/audioset/train_np/label/--PJHxphWEs.npy']\n"
     ]
    }
   ],
   "source": [
    "get_files_and_labels('/home/ysr/dataset/audio/audioset/train_np/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7902ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0DLPzsiXXE.npy', '--ekDLDTUXA.npy', '--aE2O5G5WE.npy', '--PJHxphWEs.npy', '-11LhdJgBb8.npy', '-0O3e95y4gE.npy', '--cB2ZVjpnA.npy', '-0SdAVK79lg.npy', '-0mjrMposBM.npy', '-0DdlOuIFUI.npy', '--aaILOrkII.npy']\n"
     ]
    }
   ],
   "source": [
    "tmp = os.listdir('/home/ysr/dataset/audio/audioset/train_np/labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db7da696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    'Generates YAMNet patches'\n",
    "    def __init__(self, \n",
    "                 file_list,\n",
    "                 label_list,\n",
    "                 dim = (96, 64),\n",
    "                 batch_size = 1, \n",
    "                 n_classes = 3,\n",
    "                 shuffle = True):\n",
    "        \n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.label_list) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        label_list_temp = [self.label_list[k] for k in indexes]\n",
    "        file_list_temp = [self.file_list[k] for k in indexes]\n",
    "        \n",
    "        x, y = self.__data_generation(file_list_temp, label_list_temp)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.label_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, file_list_temp, label_list_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization   \n",
    "        return np.load(file_list_temp[0]), np.load(label_list_temp[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83a02868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_train, files_val, label_train, label_val = get_files_and_labels('/home/ysr/dataset/audio/audioset/train_np/')\n",
    "datagen = DataGenerator(files_train, label_train)\n",
    "\n",
    "datagen.__getitem__(0)\n",
    "datagen.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "      #X = np.empty((self.batch_size, *self.dim))\n",
    "        X = []\n",
    "        y = np.empty((self.batch_size, self.n_classes))\n",
    "        z = np.empty((self.batch_size, 521))\n",
    "        sample_weights = np.empty((self.batch_size, ))\n",
    "        \n",
    "        y[:] = 0\n",
    "        #print(list_IDs_temp)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            #print(ID)            \n",
    "            class_id = ID.split('/')[-2]\n",
    "            #print(class_id)\n",
    "            y[i,self.labels[class_id]] = 1\n",
    "            #print(y)          \n",
    "            sample = np.load(ID)\n",
    "            \n",
    "            # if the waveform for this sample was long enough to contain multiple patches, randomly select one of the patches\n",
    "            #if sample.shape[0] > 1:\n",
    "                #sample = np.squeeze(sample[0])\n",
    "             #   sample = np.squeeze(sample[np.random.choice(range(sample.shape[0]), 1)])\n",
    "                \n",
    "            X.append(sample)\n",
    "            #X[i,] = sample\n",
    "                \n",
    "            if self.class_weights:\n",
    "                sample_weights[i] = self.class_weights[self.labels[class_id]]\n",
    "          \n",
    "        self.classes.append(y.reshape(y.shape[1]))\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            return X, y, sample_weights\n",
    "        else:\n",
    "            if self.tflite_ouput == 1:\n",
    "                return X, y\n",
    "            else:\n",
    "                return X, [y,z]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
